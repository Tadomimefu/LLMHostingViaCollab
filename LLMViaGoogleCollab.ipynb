{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMmoW+Y4y+GK1mT/I5jKub1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tm_9SR4WiKmw",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# FULL SETUP: L3-8B-Stheno-v3.2 (Q5_K_M) on Colab T4 + ngrok\n",
        "# → Make sure you selected **T4 GPU** under Runtime → Change runtime type\n",
        "# → Make sure you added the required secrets in google collab's secrets menu.\n",
        "# → RESERVED_DOMAIN = your ngrok reserved domain (should be something like certain-blalbalba-basik-ngrok-free-app)\n",
        "# → NGROK_TOKEN = your ngrok token that you get from ngrok dashboard\n",
        "# If NGROK_TOKEN doesn't work, try NGROK_AUTHTOKEN\n",
        "# CELL 1 — Verify GPU (should show Tesla T4 + CUDA 12.4)\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2 — Install everything (llama-cpp-python CUDA + server + huggingface + ngrok)\n",
        "!pip install --no-cache-dir \\\n",
        "    llama-cpp-python[server] \\\n",
        "    --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124\n",
        "\n",
        "!pip install huggingface_hub[hf_transfer] pyngrok"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZmsVzjzXqAAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3 — Download the  file (L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf)\n",
        "from huggingface_hub import hf_hub_download\n",
        "import os\n",
        "\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "\n",
        "repo_id = \"Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix\"\n",
        "\n",
        "# Choose one of these (recommended order):\n",
        "#   Q5_K_M-imat  → best quality/size balance  (~5.3 GB, ~28–32 t/s on T4)\n",
        "#   Q5_K_S-imat  → slightly smaller/faster, tiny quality drop\n",
        "#   Q6_K-imat    → noticeably better but ~6.1 GB\n",
        "#   Q8_0-imat    → reference quality but ~8.5 GB (still fits T4)\n",
        "\n",
        "chosen_file = \"L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf\"   # ← change here if you want another\n",
        "\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=repo_id,\n",
        "    filename=chosen_file,\n",
        "    local_dir=\"/content/models\",\n",
        ")\n",
        "\n",
        "print(f\"Model successfully downloaded: {chosen_file}\")\n",
        "print(f\"Path: {model_path}\")\n",
        "print(f\"Size: {os.path.getsize(model_path)/1e9:.2f} GB\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1807JSkCqMrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4 — Start the OpenAI-compatible server in the background\n",
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Full GPU offload (-1 = all layers)\n",
        "server_cmd = [\n",
        "    \"python\", \"-m\", \"llama_cpp.server\",\n",
        "    \"--model\", model_path,\n",
        "    \"--n_gpu_layers\", \"-1\",\n",
        "    \"--n_ctx\", \"8192\", # Change context size based on your LLM Choice.\n",
        "    \"--host\", \"0.0.0.0\",\n",
        "    \"--port\", \"8000\",\n",
        "    \"--n_batch\", \"512\",\n",
        "    \"--verbose\", \"true\" # Changed to true for debugging output\n",
        "]\n",
        "\n",
        "# Run server in background\n",
        "# We'll capture its stdout/stderr to files for better debugging if it crashes.\n",
        "server_log_file = \"/tmp/llama_cpp_server.log\"\n",
        "with open(server_log_file, \"w\") as log_file:\n",
        "    process = subprocess.Popen(server_cmd, stdout=log_file, stderr=log_file)\n",
        "\n",
        "# Wait for the server to start, with retries\n",
        "max_retries = 15 # Increased retries for larger models\n",
        "retry_delay = 5 # seconds between retries\n",
        "server_ready = False\n",
        "\n",
        "print(\"Waiting for llama-cpp-python server to start...\")\n",
        "for i in range(max_retries):\n",
        "    print(f\"Attempt {i+1}/{max_retries} to connect to server...\")\n",
        "    try:\n",
        "        # Use subprocess.run to capture output of curl to check server status\n",
        "        # `check=False` allows curl to fail without raising an exception immediately\n",
        "        curl_check = subprocess.run(\n",
        "            [\"curl\", \"-s\", \"http://0.0.0.0:8000/v1/models\"],\n",
        "            capture_output=True, text=True, check=False\n",
        "        )\n",
        "        # Check if the curl command was successful and returned expected content\n",
        "        if curl_check.returncode == 0 and \"object\" in curl_check.stdout:\n",
        "            print(\"Server is up and running!\")\n",
        "            server_ready = True\n",
        "            break\n",
        "        else:\n",
        "            print(f\"Server not yet ready. Curl exit code: {curl_check.returncode}. Output (truncated): {curl_check.stderr.strip()[:200] or curl_check.stdout.strip()[:200]}\")\n",
        "            time.sleep(retry_delay)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during curl check: {e}\")\n",
        "        time.sleep(retry_delay)\n",
        "\n",
        "if not server_ready:\n",
        "    print(\"\\nERROR: Server failed to start within the expected time.\")\n",
        "    print(f\"Please check the server logs in {server_log_file} for more details.\")\n",
        "    # Attempt to terminate the process if it's still running\n",
        "    if process.poll() is None: # If process is still running\n",
        "        process.terminate()\n",
        "        print(\"Background server process terminated.\")\n",
        "else:\n",
        "    print(\"\\nServer running on http://localhost:8000\")\n",
        "    # Final sanity check with curl for user confirmation\n",
        "    !curl http://0.0.0.0:8000/v1/models\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gzg6foW1qNSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "da593ec2"
      },
      "source": [
        "# CELL 4.5 - If CELL 4 wasn't able to run, check the logs by running this cell\n",
        "import os\n",
        "\n",
        "server_log_file = \"/tmp/llama_cpp_server.log\"\n",
        "\n",
        "if os.path.exists(server_log_file):\n",
        "    print(f\"--- Contents of {server_log_file} ---\")\n",
        "    with open(server_log_file, \"r\") as f:\n",
        "        print(f.read())\n",
        "    print(f\"--- End of {server_log_file} ---\")\n",
        "else:\n",
        "    print(f\"Server log file not found at {server_log_file}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5 — Expose with your RESERVED ngrok domain (always the same URL)\n",
        "from google.colab import userdata\n",
        "from pyngrok import ngrok\n",
        "import time\n",
        "\n",
        "# Your ngrok auth token\n",
        "ngrok.set_auth_token(userdata.get('NGROK_TOKEN'))\n",
        "\n",
        "# Your ngrok reserved domain\n",
        "RESERVED_DOMAIN=(userdata.get('RESERVED_DOMAIN'))\n",
        "\n",
        "# Kill any old tunnels on port 8000 just in case\n",
        "for tunnel in ngrok.get_tunnels():\n",
        "    if tunnel.config.get(\"addr\") == \"localhost:8000\":\n",
        "        ngrok.disconnect(tunnel.public_url)\n",
        "\n",
        "# Open the tunnel with your exact reserved domain\n",
        "tunnel = ngrok.connect(\n",
        "    addr=\"8000\",\n",
        "    proto=\"http\",\n",
        "    bind_tls=True,\n",
        "    domain=RESERVED_DOMAIN          # ← this locks it to your domain\n",
        ")\n",
        "\n",
        "public_url = tunnel.public_url\n",
        "print(\"\\nYOUR PUBLIC ENDPOINT (OpenAI compatible):\")\n",
        "print(public_url)\n",
        "print(\"\\nExample curl:\")\n",
        "print(f'curl {public_url}/v1/chat/completions \\\\\\n'\n",
        "      '  -H \"Content-Type: application/json\" \\\\\\n'\n",
        "      '  -d \\'{{\"model\":\"llama\",\"messages\":[{{\"role\":\"user\",\"content\":\"Hello Stheno!\"}}],\"temperature\":0.8}}\\'')\n",
        "\n",
        "print(\"\\nServer is live at your permanent URL above!\")\n",
        "print(\"You can close the notebook — the tunnel stays alive as long as this cell keeps running.\\n\")\n",
        "\n",
        "# Keep the cell alive forever\n",
        "while True:\n",
        "    time.sleep(60)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Q_p1cZvUqQ6c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}