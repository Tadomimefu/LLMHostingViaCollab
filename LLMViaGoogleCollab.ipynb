{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPfxchkcSWttl1nEolNO4k+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tadomimefu/LLMHostingViaCollab/blob/main/LLMViaGoogleCollab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "tm_9SR4WiKmw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c782d980-54f0-4a94-abc0-addc4db241a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Dec  5 19:06:17 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# FULL SETUP: L3-8B-Stheno-v3.2 (Q5_K_M) on Colab T4 + ngrok\n",
        "# → Make sure you selected **T4 GPU** under Runtime → Change runtime type\n",
        "# → Make sure you added the required secrets in google collab's secrets menu.\n",
        "# → RESERVED_DOMAIN = your ngrok reserved domain (should be something like certain-blalbalba-basik-ngrok-free-app)\n",
        "# → NGROK_TOKEN = your ngrok token that you get from ngrok dashboard\n",
        "# If NGROK_TOKEN doesn't work, try NGROK_AUTHTOKEN\n",
        "# CELL 1 — Verify GPU (should show Tesla T4 + CUDA 12.4)\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2 — Install everything (llama-cpp-python CUDA + server + huggingface + ngrok)\n",
        "!pip install --no-cache-dir \\\n",
        "    llama-cpp-python[server] \\\n",
        "    --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124\n",
        "\n",
        "!pip install huggingface_hub[hf_transfer] pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmsVzjzXqAAo",
        "outputId": "b34eafa6-315e-4fa7-a4fd-d4415c724636"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu124\n",
            "Requirement already satisfied: llama-cpp-python[server] in /usr/local/lib/python3.12/dist-packages (0.3.16)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python[server]) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python[server]) (2.3.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python[server]) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python[server]) (3.1.6)\n",
            "Requirement already satisfied: uvicorn>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python[server]) (0.38.0)\n",
            "Requirement already satisfied: fastapi>=0.100.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python[server]) (0.123.9)\n",
            "Requirement already satisfied: pydantic-settings>=2.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python[server]) (2.12.0)\n",
            "Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python[server]) (3.0.3)\n",
            "Requirement already satisfied: starlette-context<0.4,>=0.3.6 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python[server]) (0.3.6)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python[server]) (6.0.3)\n",
            "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.100.0->llama-cpp-python[server]) (0.50.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.100.0->llama-cpp-python[server]) (2.12.5)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.100.0->llama-cpp-python[server]) (0.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.11.3->llama-cpp-python[server]) (3.0.3)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings>=2.0.1->llama-cpp-python[server]) (1.2.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings>=2.0.1->llama-cpp-python[server]) (0.4.2)\n",
            "Requirement already satisfied: anyio>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from sse-starlette>=1.6.1->llama-cpp-python[server]) (4.12.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.22.0->llama-cpp-python[server]) (8.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.22.0->llama-cpp-python[server]) (0.16.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio>=4.7.0->sse-starlette>=1.6.1->llama-cpp-python[server]) (3.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.100.0->llama-cpp-python[server]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.100.0->llama-cpp-python[server]) (2.41.5)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.5.0)\n",
            "Requirement already satisfied: huggingface_hub[hf_transfer] in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[hf_transfer]) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[hf_transfer]) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[hf_transfer]) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[hf_transfer]) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[hf_transfer]) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[hf_transfer]) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[hf_transfer]) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[hf_transfer]) (1.2.0)\n",
            "Requirement already satisfied: hf-transfer>=0.1.4 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[hf_transfer]) (0.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub[hf_transfer]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub[hf_transfer]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub[hf_transfer]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub[hf_transfer]) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3 — Download the  file (L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf)\n",
        "from huggingface_hub import hf_hub_download\n",
        "import os\n",
        "\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "\n",
        "repo_id = \"Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix\"\n",
        "\n",
        "# Choose one of these (recommended order):\n",
        "#   Q5_K_M-imat  → best quality/size balance  (~5.3 GB, ~28–32 t/s on T4)\n",
        "#   Q5_K_S-imat  → slightly smaller/faster, tiny quality drop\n",
        "#   Q6_K-imat    → noticeably better but ~6.1 GB\n",
        "#   Q8_0-imat    → reference quality but ~8.5 GB (still fits T4)\n",
        "\n",
        "chosen_file = \"L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf\"   # ← change here if you want another\n",
        "\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=repo_id,\n",
        "    filename=chosen_file,\n",
        "    local_dir=\"/content/models\",\n",
        "    local_dir_use_symlinks=False\n",
        ")\n",
        "\n",
        "print(f\"Model successfully downloaded: {chosen_file}\")\n",
        "print(f\"Path: {model_path}\")\n",
        "print(f\"Size: {os.path.getsize(model_path)/1e9:.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1807JSkCqMrf",
        "outputId": "3192bab3-f7ea-45f0-eedb-dc7757cb0a00"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model successfully downloaded: L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf\n",
            "Path: /content/models/L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf\n",
            "Size: 5.73 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
            "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4 — Start the OpenAI-compatible server in the background\n",
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Full GPU offload (-1 = all layers)\n",
        "server_cmd = [\n",
        "    \"python\", \"-m\", \"llama_cpp.server\",\n",
        "    \"--model\", model_path,\n",
        "    \"--n_gpu_layers\", \"-1\",\n",
        "    \"--n_ctx\", \"8192\",\n",
        "    \"--host\", \"0.0.0.0\",\n",
        "    \"--port\", \"8000\",\n",
        "    \"--n_batch\", \"512\",\n",
        "    \"--verbose\", \"true\" # Changed to true for debugging output\n",
        "]\n",
        "\n",
        "# Run server in background\n",
        "# We'll capture its stdout/stderr to files for better debugging if it crashes.\n",
        "server_log_file = \"/tmp/llama_cpp_server.log\"\n",
        "with open(server_log_file, \"w\") as log_file:\n",
        "    process = subprocess.Popen(server_cmd, stdout=log_file, stderr=log_file)\n",
        "\n",
        "# Wait for the server to start, with retries\n",
        "max_retries = 15 # Increased retries for larger models\n",
        "retry_delay = 5 # seconds between retries\n",
        "server_ready = False\n",
        "\n",
        "print(\"Waiting for llama-cpp-python server to start...\")\n",
        "for i in range(max_retries):\n",
        "    print(f\"Attempt {i+1}/{max_retries} to connect to server...\")\n",
        "    try:\n",
        "        # Use subprocess.run to capture output of curl to check server status\n",
        "        # `check=False` allows curl to fail without raising an exception immediately\n",
        "        curl_check = subprocess.run(\n",
        "            [\"curl\", \"-s\", \"http://0.0.0.0:8000/v1/models\"],\n",
        "            capture_output=True, text=True, check=False\n",
        "        )\n",
        "        # Check if the curl command was successful and returned expected content\n",
        "        if curl_check.returncode == 0 and \"object\" in curl_check.stdout:\n",
        "            print(\"Server is up and running!\")\n",
        "            server_ready = True\n",
        "            break\n",
        "        else:\n",
        "            print(f\"Server not yet ready. Curl exit code: {curl_check.returncode}. Output (truncated): {curl_check.stderr.strip()[:200] or curl_check.stdout.strip()[:200]}\")\n",
        "            time.sleep(retry_delay)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during curl check: {e}\")\n",
        "        time.sleep(retry_delay)\n",
        "\n",
        "if not server_ready:\n",
        "    print(\"\\nERROR: Server failed to start within the expected time.\")\n",
        "    print(f\"Please check the server logs in {server_log_file} for more details.\")\n",
        "    # Attempt to terminate the process if it's still running\n",
        "    if process.poll() is None: # If process is still running\n",
        "        process.terminate()\n",
        "        print(\"Background server process terminated.\")\n",
        "else:\n",
        "    print(\"\\nServer running on http://localhost:8000\")\n",
        "    # Final sanity check with curl for user confirmation\n",
        "    !curl http://0.0.0.0:8000/v1/models\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzg6foW1qNSp",
        "outputId": "c29b7cff-b43c-40fb-908e-12b4e4d15a2e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting for llama-cpp-python server to start...\n",
            "Attempt 1/15 to connect to server...\n",
            "Server not yet ready. Curl exit code: 7. Output (truncated): \n",
            "Attempt 2/15 to connect to server...\n",
            "Server is up and running!\n",
            "\n",
            "Server running on http://localhost:8000\n",
            "{\"object\":\"list\",\"data\":[{\"id\":\"/content/models/L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf\",\"object\":\"model\",\"owned_by\":\"me\",\"permissions\":[]}]}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "da593ec2",
        "outputId": "26ff4edd-0ec7-4a13-e6a7-a84d609200f0"
      },
      "source": [
        "# CELL 4.5 - If CELL 5 wasn't able to run, check the logs by running this cell\n",
        "import os\n",
        "\n",
        "server_log_file = \"/tmp/llama_cpp_server.log\"\n",
        "\n",
        "if os.path.exists(server_log_file):\n",
        "    print(f\"--- Contents of {server_log_file} ---\")\n",
        "    with open(server_log_file, \"r\") as f:\n",
        "        print(f.read())\n",
        "    print(f\"--- End of {server_log_file} ---\")\n",
        "else:\n",
        "    print(f\"Server log file not found at {server_log_file}\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Contents of /tmp/llama_cpp_server.log ---\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 26 key-value pairs and 291 tensors from /content/models/L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = L3-8B-Stheno-v3.2\n",
            "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
            "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  22:                      quantize.imatrix.file str              = E:\\Ferramentas\\gguf-quantizations\\con...\n",
            "llama_model_loader: - kv  23:                   quantize.imatrix.dataset str              = E:\\Ferramentas\\gguf-quantizations\\con...\n",
            "llama_model_loader: - kv  24:             quantize.imatrix.entries_count i32              = 224\n",
            "llama_model_loader: - kv  25:              quantize.imatrix.chunks_count i32              = 101\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q5_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q5_K - Medium\n",
            "print_info: file size   = 5.33 GiB (5.70 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token: 128255 '<|reserved_special_token_250|>' is not marked as EOG\n",
            "load: control token: 128254 '<|reserved_special_token_249|>' is not marked as EOG\n",
            "load: control token: 128253 '<|reserved_special_token_248|>' is not marked as EOG\n",
            "load: control token: 128251 '<|reserved_special_token_246|>' is not marked as EOG\n",
            "load: control token: 128246 '<|reserved_special_token_241|>' is not marked as EOG\n",
            "load: control token: 128243 '<|reserved_special_token_238|>' is not marked as EOG\n",
            "load: control token: 128240 '<|reserved_special_token_235|>' is not marked as EOG\n",
            "load: control token: 128239 '<|reserved_special_token_234|>' is not marked as EOG\n",
            "load: control token: 128238 '<|reserved_special_token_233|>' is not marked as EOG\n",
            "load: control token: 128237 '<|reserved_special_token_232|>' is not marked as EOG\n",
            "load: control token: 128232 '<|reserved_special_token_227|>' is not marked as EOG\n",
            "load: control token: 128228 '<|reserved_special_token_223|>' is not marked as EOG\n",
            "load: control token: 128227 '<|reserved_special_token_222|>' is not marked as EOG\n",
            "load: control token: 128225 '<|reserved_special_token_220|>' is not marked as EOG\n",
            "load: control token: 128222 '<|reserved_special_token_217|>' is not marked as EOG\n",
            "load: control token: 128215 '<|reserved_special_token_210|>' is not marked as EOG\n",
            "load: control token: 128211 '<|reserved_special_token_206|>' is not marked as EOG\n",
            "load: control token: 128210 '<|reserved_special_token_205|>' is not marked as EOG\n",
            "load: control token: 128204 '<|reserved_special_token_199|>' is not marked as EOG\n",
            "load: control token: 128203 '<|reserved_special_token_198|>' is not marked as EOG\n",
            "load: control token: 128201 '<|reserved_special_token_196|>' is not marked as EOG\n",
            "load: control token: 128197 '<|reserved_special_token_192|>' is not marked as EOG\n",
            "load: control token: 128196 '<|reserved_special_token_191|>' is not marked as EOG\n",
            "load: control token: 128195 '<|reserved_special_token_190|>' is not marked as EOG\n",
            "load: control token: 128193 '<|reserved_special_token_188|>' is not marked as EOG\n",
            "load: control token: 128191 '<|reserved_special_token_186|>' is not marked as EOG\n",
            "load: control token: 128190 '<|reserved_special_token_185|>' is not marked as EOG\n",
            "load: control token: 128185 '<|reserved_special_token_180|>' is not marked as EOG\n",
            "load: control token: 128184 '<|reserved_special_token_179|>' is not marked as EOG\n",
            "load: control token: 128182 '<|reserved_special_token_177|>' is not marked as EOG\n",
            "load: control token: 128181 '<|reserved_special_token_176|>' is not marked as EOG\n",
            "load: control token: 128177 '<|reserved_special_token_172|>' is not marked as EOG\n",
            "load: control token: 128176 '<|reserved_special_token_171|>' is not marked as EOG\n",
            "load: control token: 128175 '<|reserved_special_token_170|>' is not marked as EOG\n",
            "load: control token: 128174 '<|reserved_special_token_169|>' is not marked as EOG\n",
            "load: control token: 128173 '<|reserved_special_token_168|>' is not marked as EOG\n",
            "load: control token: 128172 '<|reserved_special_token_167|>' is not marked as EOG\n",
            "load: control token: 128168 '<|reserved_special_token_163|>' is not marked as EOG\n",
            "load: control token: 128167 '<|reserved_special_token_162|>' is not marked as EOG\n",
            "load: control token: 128166 '<|reserved_special_token_161|>' is not marked as EOG\n",
            "load: control token: 128165 '<|reserved_special_token_160|>' is not marked as EOG\n",
            "load: control token: 128162 '<|reserved_special_token_157|>' is not marked as EOG\n",
            "load: control token: 128159 '<|reserved_special_token_154|>' is not marked as EOG\n",
            "load: control token: 128155 '<|reserved_special_token_150|>' is not marked as EOG\n",
            "load: control token: 128153 '<|reserved_special_token_148|>' is not marked as EOG\n",
            "load: control token: 128152 '<|reserved_special_token_147|>' is not marked as EOG\n",
            "load: control token: 128151 '<|reserved_special_token_146|>' is not marked as EOG\n",
            "load: control token: 128148 '<|reserved_special_token_143|>' is not marked as EOG\n",
            "load: control token: 128146 '<|reserved_special_token_141|>' is not marked as EOG\n",
            "load: control token: 128144 '<|reserved_special_token_139|>' is not marked as EOG\n",
            "load: control token: 128143 '<|reserved_special_token_138|>' is not marked as EOG\n",
            "load: control token: 128141 '<|reserved_special_token_136|>' is not marked as EOG\n",
            "load: control token: 128139 '<|reserved_special_token_134|>' is not marked as EOG\n",
            "load: control token: 128138 '<|reserved_special_token_133|>' is not marked as EOG\n",
            "load: control token: 128135 '<|reserved_special_token_130|>' is not marked as EOG\n",
            "load: control token: 128133 '<|reserved_special_token_128|>' is not marked as EOG\n",
            "load: control token: 128132 '<|reserved_special_token_127|>' is not marked as EOG\n",
            "load: control token: 128131 '<|reserved_special_token_126|>' is not marked as EOG\n",
            "load: control token: 128130 '<|reserved_special_token_125|>' is not marked as EOG\n",
            "load: control token: 128128 '<|reserved_special_token_123|>' is not marked as EOG\n",
            "load: control token: 128125 '<|reserved_special_token_120|>' is not marked as EOG\n",
            "load: control token: 128121 '<|reserved_special_token_116|>' is not marked as EOG\n",
            "load: control token: 128120 '<|reserved_special_token_115|>' is not marked as EOG\n",
            "load: control token: 128119 '<|reserved_special_token_114|>' is not marked as EOG\n",
            "load: control token: 128116 '<|reserved_special_token_111|>' is not marked as EOG\n",
            "load: control token: 128112 '<|reserved_special_token_107|>' is not marked as EOG\n",
            "load: control token: 128109 '<|reserved_special_token_104|>' is not marked as EOG\n",
            "load: control token: 128107 '<|reserved_special_token_102|>' is not marked as EOG\n",
            "load: control token: 128106 '<|reserved_special_token_101|>' is not marked as EOG\n",
            "load: control token: 128105 '<|reserved_special_token_100|>' is not marked as EOG\n",
            "load: control token: 128103 '<|reserved_special_token_98|>' is not marked as EOG\n",
            "load: control token: 128100 '<|reserved_special_token_95|>' is not marked as EOG\n",
            "load: control token: 128099 '<|reserved_special_token_94|>' is not marked as EOG\n",
            "load: control token: 128098 '<|reserved_special_token_93|>' is not marked as EOG\n",
            "load: control token: 128094 '<|reserved_special_token_89|>' is not marked as EOG\n",
            "load: control token: 128088 '<|reserved_special_token_83|>' is not marked as EOG\n",
            "load: control token: 128087 '<|reserved_special_token_82|>' is not marked as EOG\n",
            "load: control token: 128086 '<|reserved_special_token_81|>' is not marked as EOG\n",
            "load: control token: 128084 '<|reserved_special_token_79|>' is not marked as EOG\n",
            "load: control token: 128082 '<|reserved_special_token_77|>' is not marked as EOG\n",
            "load: control token: 128078 '<|reserved_special_token_73|>' is not marked as EOG\n",
            "load: control token: 128075 '<|reserved_special_token_70|>' is not marked as EOG\n",
            "load: control token: 128073 '<|reserved_special_token_68|>' is not marked as EOG\n",
            "load: control token: 128072 '<|reserved_special_token_67|>' is not marked as EOG\n",
            "load: control token: 128070 '<|reserved_special_token_65|>' is not marked as EOG\n",
            "load: control token: 128065 '<|reserved_special_token_60|>' is not marked as EOG\n",
            "load: control token: 128064 '<|reserved_special_token_59|>' is not marked as EOG\n",
            "load: control token: 128062 '<|reserved_special_token_57|>' is not marked as EOG\n",
            "load: control token: 128060 '<|reserved_special_token_55|>' is not marked as EOG\n",
            "load: control token: 128059 '<|reserved_special_token_54|>' is not marked as EOG\n",
            "load: control token: 128057 '<|reserved_special_token_52|>' is not marked as EOG\n",
            "load: control token: 128056 '<|reserved_special_token_51|>' is not marked as EOG\n",
            "load: control token: 128054 '<|reserved_special_token_49|>' is not marked as EOG\n",
            "load: control token: 128051 '<|reserved_special_token_46|>' is not marked as EOG\n",
            "load: control token: 128043 '<|reserved_special_token_38|>' is not marked as EOG\n",
            "load: control token: 128042 '<|reserved_special_token_37|>' is not marked as EOG\n",
            "load: control token: 128041 '<|reserved_special_token_36|>' is not marked as EOG\n",
            "load: control token: 128040 '<|reserved_special_token_35|>' is not marked as EOG\n",
            "load: control token: 128035 '<|reserved_special_token_30|>' is not marked as EOG\n",
            "load: control token: 128033 '<|reserved_special_token_28|>' is not marked as EOG\n",
            "load: control token: 128032 '<|reserved_special_token_27|>' is not marked as EOG\n",
            "load: control token: 128029 '<|reserved_special_token_24|>' is not marked as EOG\n",
            "load: control token: 128025 '<|reserved_special_token_20|>' is not marked as EOG\n",
            "load: control token: 128024 '<|reserved_special_token_19|>' is not marked as EOG\n",
            "load: control token: 128021 '<|reserved_special_token_16|>' is not marked as EOG\n",
            "load: control token: 128020 '<|reserved_special_token_15|>' is not marked as EOG\n",
            "load: control token: 128019 '<|reserved_special_token_14|>' is not marked as EOG\n",
            "load: control token: 128018 '<|reserved_special_token_13|>' is not marked as EOG\n",
            "load: control token: 128015 '<|reserved_special_token_10|>' is not marked as EOG\n",
            "load: control token: 128013 '<|reserved_special_token_8|>' is not marked as EOG\n",
            "load: control token: 128012 '<|reserved_special_token_7|>' is not marked as EOG\n",
            "load: control token: 128010 '<|reserved_special_token_5|>' is not marked as EOG\n",
            "load: control token: 128005 '<|reserved_special_token_3|>' is not marked as EOG\n",
            "load: control token: 128004 '<|reserved_special_token_2|>' is not marked as EOG\n",
            "load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
            "load: control token: 128249 '<|reserved_special_token_244|>' is not marked as EOG\n",
            "load: control token: 128187 '<|reserved_special_token_182|>' is not marked as EOG\n",
            "load: control token: 128180 '<|reserved_special_token_175|>' is not marked as EOG\n",
            "load: control token: 128134 '<|reserved_special_token_129|>' is not marked as EOG\n",
            "load: control token: 128179 '<|reserved_special_token_174|>' is not marked as EOG\n",
            "load: control token: 128037 '<|reserved_special_token_32|>' is not marked as EOG\n",
            "load: control token: 128045 '<|reserved_special_token_40|>' is not marked as EOG\n",
            "load: control token: 128089 '<|reserved_special_token_84|>' is not marked as EOG\n",
            "load: control token: 128212 '<|reserved_special_token_207|>' is not marked as EOG\n",
            "load: control token: 128104 '<|reserved_special_token_99|>' is not marked as EOG\n",
            "load: control token: 128205 '<|reserved_special_token_200|>' is not marked as EOG\n",
            "load: control token: 128142 '<|reserved_special_token_137|>' is not marked as EOG\n",
            "load: control token: 128028 '<|reserved_special_token_23|>' is not marked as EOG\n",
            "load: control token: 128126 '<|reserved_special_token_121|>' is not marked as EOG\n",
            "load: control token: 128198 '<|reserved_special_token_193|>' is not marked as EOG\n",
            "load: control token: 128071 '<|reserved_special_token_66|>' is not marked as EOG\n",
            "load: control token: 128092 '<|reserved_special_token_87|>' is not marked as EOG\n",
            "load: control token: 128183 '<|reserved_special_token_178|>' is not marked as EOG\n",
            "load: control token: 128140 '<|reserved_special_token_135|>' is not marked as EOG\n",
            "load: control token: 128226 '<|reserved_special_token_221|>' is not marked as EOG\n",
            "load: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
            "load: control token: 128052 '<|reserved_special_token_47|>' is not marked as EOG\n",
            "load: control token: 128053 '<|reserved_special_token_48|>' is not marked as EOG\n",
            "load: control token: 128058 '<|reserved_special_token_53|>' is not marked as EOG\n",
            "load: control token: 128150 '<|reserved_special_token_145|>' is not marked as EOG\n",
            "load: control token: 128149 '<|reserved_special_token_144|>' is not marked as EOG\n",
            "load: control token: 128209 '<|reserved_special_token_204|>' is not marked as EOG\n",
            "load: control token: 128169 '<|reserved_special_token_164|>' is not marked as EOG\n",
            "load: control token: 128157 '<|reserved_special_token_152|>' is not marked as EOG\n",
            "load: control token: 128038 '<|reserved_special_token_33|>' is not marked as EOG\n",
            "load: control token: 128178 '<|reserved_special_token_173|>' is not marked as EOG\n",
            "load: control token: 128091 '<|reserved_special_token_86|>' is not marked as EOG\n",
            "load: control token: 128115 '<|reserved_special_token_110|>' is not marked as EOG\n",
            "load: control token: 128233 '<|reserved_special_token_228|>' is not marked as EOG\n",
            "load: control token: 128145 '<|reserved_special_token_140|>' is not marked as EOG\n",
            "load: control token: 128039 '<|reserved_special_token_34|>' is not marked as EOG\n",
            "load: control token: 128136 '<|reserved_special_token_131|>' is not marked as EOG\n",
            "load: control token: 128170 '<|reserved_special_token_165|>' is not marked as EOG\n",
            "load: control token: 128236 '<|reserved_special_token_231|>' is not marked as EOG\n",
            "load: control token: 128154 '<|reserved_special_token_149|>' is not marked as EOG\n",
            "load: control token: 128049 '<|reserved_special_token_44|>' is not marked as EOG\n",
            "load: control token: 128023 '<|reserved_special_token_18|>' is not marked as EOG\n",
            "load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
            "load: control token: 128016 '<|reserved_special_token_11|>' is not marked as EOG\n",
            "load: control token: 128113 '<|reserved_special_token_108|>' is not marked as EOG\n",
            "load: control token: 128158 '<|reserved_special_token_153|>' is not marked as EOG\n",
            "load: control token: 128223 '<|reserved_special_token_218|>' is not marked as EOG\n",
            "load: control token: 128156 '<|reserved_special_token_151|>' is not marked as EOG\n",
            "load: control token: 128008 '<|reserved_special_token_4|>' is not marked as EOG\n",
            "load: control token: 128085 '<|reserved_special_token_80|>' is not marked as EOG\n",
            "load: control token: 128160 '<|reserved_special_token_155|>' is not marked as EOG\n",
            "load: control token: 128110 '<|reserved_special_token_105|>' is not marked as EOG\n",
            "load: control token: 128247 '<|reserved_special_token_242|>' is not marked as EOG\n",
            "load: control token: 128122 '<|reserved_special_token_117|>' is not marked as EOG\n",
            "load: control token: 128050 '<|reserved_special_token_45|>' is not marked as EOG\n",
            "load: control token: 128221 '<|reserved_special_token_216|>' is not marked as EOG\n",
            "load: control token: 128244 '<|reserved_special_token_239|>' is not marked as EOG\n",
            "load: control token: 128248 '<|reserved_special_token_243|>' is not marked as EOG\n",
            "load: control token: 128213 '<|reserved_special_token_208|>' is not marked as EOG\n",
            "load: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
            "load: control token: 128208 '<|reserved_special_token_203|>' is not marked as EOG\n",
            "load: control token: 128074 '<|reserved_special_token_69|>' is not marked as EOG\n",
            "load: control token: 128234 '<|reserved_special_token_229|>' is not marked as EOG\n",
            "load: control token: 128083 '<|reserved_special_token_78|>' is not marked as EOG\n",
            "load: control token: 128224 '<|reserved_special_token_219|>' is not marked as EOG\n",
            "load: control token: 128055 '<|reserved_special_token_50|>' is not marked as EOG\n",
            "load: control token: 128097 '<|reserved_special_token_92|>' is not marked as EOG\n",
            "load: control token: 128206 '<|reserved_special_token_201|>' is not marked as EOG\n",
            "load: control token: 128081 '<|reserved_special_token_76|>' is not marked as EOG\n",
            "load: control token: 128068 '<|reserved_special_token_63|>' is not marked as EOG\n",
            "load: control token: 128067 '<|reserved_special_token_62|>' is not marked as EOG\n",
            "load: control token: 128046 '<|reserved_special_token_41|>' is not marked as EOG\n",
            "load: control token: 128194 '<|reserved_special_token_189|>' is not marked as EOG\n",
            "load: control token: 128069 '<|reserved_special_token_64|>' is not marked as EOG\n",
            "load: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
            "load: control token: 128220 '<|reserved_special_token_215|>' is not marked as EOG\n",
            "load: control token: 128214 '<|reserved_special_token_209|>' is not marked as EOG\n",
            "load: control token: 128108 '<|reserved_special_token_103|>' is not marked as EOG\n",
            "load: control token: 128200 '<|reserved_special_token_195|>' is not marked as EOG\n",
            "load: control token: 128048 '<|reserved_special_token_43|>' is not marked as EOG\n",
            "load: control token: 128027 '<|reserved_special_token_22|>' is not marked as EOG\n",
            "load: control token: 128114 '<|reserved_special_token_109|>' is not marked as EOG\n",
            "load: control token: 128235 '<|reserved_special_token_230|>' is not marked as EOG\n",
            "load: control token: 128252 '<|reserved_special_token_247|>' is not marked as EOG\n",
            "load: control token: 128199 '<|reserved_special_token_194|>' is not marked as EOG\n",
            "load: control token: 128129 '<|reserved_special_token_124|>' is not marked as EOG\n",
            "load: control token: 128245 '<|reserved_special_token_240|>' is not marked as EOG\n",
            "load: control token: 128164 '<|reserved_special_token_159|>' is not marked as EOG\n",
            "load: control token: 128124 '<|reserved_special_token_119|>' is not marked as EOG\n",
            "load: control token: 128102 '<|reserved_special_token_97|>' is not marked as EOG\n",
            "load: control token: 128036 '<|reserved_special_token_31|>' is not marked as EOG\n",
            "load: control token: 128229 '<|reserved_special_token_224|>' is not marked as EOG\n",
            "load: control token: 128163 '<|reserved_special_token_158|>' is not marked as EOG\n",
            "load: control token: 128127 '<|reserved_special_token_122|>' is not marked as EOG\n",
            "load: control token: 128111 '<|reserved_special_token_106|>' is not marked as EOG\n",
            "load: control token: 128231 '<|reserved_special_token_226|>' is not marked as EOG\n",
            "load: control token: 128188 '<|reserved_special_token_183|>' is not marked as EOG\n",
            "load: control token: 128061 '<|reserved_special_token_56|>' is not marked as EOG\n",
            "load: control token: 128137 '<|reserved_special_token_132|>' is not marked as EOG\n",
            "load: control token: 128093 '<|reserved_special_token_88|>' is not marked as EOG\n",
            "load: control token: 128095 '<|reserved_special_token_90|>' is not marked as EOG\n",
            "load: control token: 128189 '<|reserved_special_token_184|>' is not marked as EOG\n",
            "load: control token: 128090 '<|reserved_special_token_85|>' is not marked as EOG\n",
            "load: control token: 128147 '<|reserved_special_token_142|>' is not marked as EOG\n",
            "load: control token: 128219 '<|reserved_special_token_214|>' is not marked as EOG\n",
            "load: control token: 128230 '<|reserved_special_token_225|>' is not marked as EOG\n",
            "load: control token: 128217 '<|reserved_special_token_212|>' is not marked as EOG\n",
            "load: control token: 128031 '<|reserved_special_token_26|>' is not marked as EOG\n",
            "load: control token: 128030 '<|reserved_special_token_25|>' is not marked as EOG\n",
            "load: control token: 128250 '<|reserved_special_token_245|>' is not marked as EOG\n",
            "load: control token: 128192 '<|reserved_special_token_187|>' is not marked as EOG\n",
            "load: control token: 128096 '<|reserved_special_token_91|>' is not marked as EOG\n",
            "load: control token: 128186 '<|reserved_special_token_181|>' is not marked as EOG\n",
            "load: control token: 128207 '<|reserved_special_token_202|>' is not marked as EOG\n",
            "load: control token: 128171 '<|reserved_special_token_166|>' is not marked as EOG\n",
            "load: control token: 128080 '<|reserved_special_token_75|>' is not marked as EOG\n",
            "load: control token: 128077 '<|reserved_special_token_72|>' is not marked as EOG\n",
            "load: control token: 128101 '<|reserved_special_token_96|>' is not marked as EOG\n",
            "load: control token: 128079 '<|reserved_special_token_74|>' is not marked as EOG\n",
            "load: control token: 128216 '<|reserved_special_token_211|>' is not marked as EOG\n",
            "load: control token: 128014 '<|reserved_special_token_9|>' is not marked as EOG\n",
            "load: control token: 128047 '<|reserved_special_token_42|>' is not marked as EOG\n",
            "load: control token: 128202 '<|reserved_special_token_197|>' is not marked as EOG\n",
            "load: control token: 128044 '<|reserved_special_token_39|>' is not marked as EOG\n",
            "load: control token: 128161 '<|reserved_special_token_156|>' is not marked as EOG\n",
            "load: control token: 128017 '<|reserved_special_token_12|>' is not marked as EOG\n",
            "load: control token: 128066 '<|reserved_special_token_61|>' is not marked as EOG\n",
            "load: control token: 128242 '<|reserved_special_token_237|>' is not marked as EOG\n",
            "load: control token: 128118 '<|reserved_special_token_113|>' is not marked as EOG\n",
            "load: control token: 128076 '<|reserved_special_token_71|>' is not marked as EOG\n",
            "load: control token: 128034 '<|reserved_special_token_29|>' is not marked as EOG\n",
            "load: control token: 128241 '<|reserved_special_token_236|>' is not marked as EOG\n",
            "load: control token: 128026 '<|reserved_special_token_21|>' is not marked as EOG\n",
            "load: control token: 128218 '<|reserved_special_token_213|>' is not marked as EOG\n",
            "load: control token: 128063 '<|reserved_special_token_58|>' is not marked as EOG\n",
            "load: control token: 128117 '<|reserved_special_token_112|>' is not marked as EOG\n",
            "load: control token: 128011 '<|reserved_special_token_6|>' is not marked as EOG\n",
            "load: control token: 128022 '<|reserved_special_token_17|>' is not marked as EOG\n",
            "load: control token: 128123 '<|reserved_special_token_118|>' is not marked as EOG\n",
            "load: printing all EOG tokens:\n",
            "load:   - 128001 ('<|end_of_text|>')\n",
            "load:   - 128009 ('<|eot_id|>')\n",
            "load: special tokens cache size = 256\n",
            "load: token to piece cache size = 0.8000 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 8192\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 4\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 14336\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 500000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 8192\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 8B\n",
            "print_info: model params     = 8.03 B\n",
            "print_info: general.name     = L3-8B-Stheno-v3.2\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 128256\n",
            "print_info: n_merges         = 280147\n",
            "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
            "print_info: EOS token        = 128009 '<|eot_id|>'\n",
            "print_info: EOT token        = 128009 '<|eot_id|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: EOG token        = 128001 '<|end_of_text|>'\n",
            "print_info: EOG token        = 128009 '<|eot_id|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  29 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  30 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  31 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  32 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q5_K) (and 0 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead\n",
            "load_tensors: offloading 32 repeating layers to GPU\n",
            "load_tensors: offloading output layer to GPU\n",
            "load_tensors: offloaded 33/33 layers to GPU\n",
            "load_tensors:        CUDA0 model buffer size =  5115.49 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =   344.44 MiB\n",
            ".......................................................................................warning: failed to mlock 799948800-byte buffer (after previously locking 0 bytes): Cannot allocate memory\n",
            "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
            ".\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 8192\n",
            "llama_context: n_ctx_per_seq = 8192\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 500000.0\n",
            "llama_context: freq_scale    = 1\n",
            "set_abort_callback: call\n",
            "llama_context:  CUDA_Host  output buffer size =     0.49 MiB\n",
            "create_memory: n_ctx = 8192 (padded)\n",
            "llama_kv_cache_unified: layer   0: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   1: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   2: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   3: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   4: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   5: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   6: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   7: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   8: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   9: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  10: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  11: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  12: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  13: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  14: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  15: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  16: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  17: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  18: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  19: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  20: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  21: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  22: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  23: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  24: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  25: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  26: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  27: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  28: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  29: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  30: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  31: dev = CUDA0\n",
            "llama_kv_cache_unified:      CUDA0 KV buffer size =  1024.00 MiB\n",
            "llama_kv_cache_unified: size = 1024.00 MiB (  8192 cells,  32 layers,  1/1 seqs), K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 2\n",
            "llama_context: max_nodes = 2328\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:      CUDA0 compute buffer size =   564.01 MiB\n",
            "llama_context:  CUDA_Host compute buffer size =    28.01 MiB\n",
            "llama_context: graph nodes  = 1126\n",
            "llama_context: graph splits = 2\n",
            "CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "Model metadata: {'quantize.imatrix.entries_count': '224', 'quantize.imatrix.dataset': 'E:\\\\Ferramentas\\\\gguf-quantizations\\\\conda\\\\imatrix\\\\imatrix.txt', 'quantize.imatrix.chunks_count': '101', 'quantize.imatrix.file': 'E:\\\\Ferramentas\\\\gguf-quantizations\\\\conda\\\\models\\\\L3-8B-Stheno-v3.2-GGUF\\\\imatrix.dat', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '8192', 'general.name': 'L3-8B-Stheno-v3.2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.file_type': '17', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Guessed chat format: llama-3\n",
            "INFO:     Started server process [25371]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
            "INFO:     127.0.0.1:45636 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "INFO:     127.0.0.1:45648 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "INFO:     103.28.116.212:0 - \"OPTIONS /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "llama_perf_context_print:        load time =     364.20 ms\n",
            "llama_perf_context_print: prompt eval time =     364.02 ms /    13 tokens (   28.00 ms per token,    35.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =     117.37 ms /     1 runs   (  117.37 ms per token,     8.52 tokens per second)\n",
            "llama_perf_context_print:       total time =     497.82 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "INFO:     103.28.116.212:0 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "INFO:     103.28.116.212:0 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "Llama.generate: 2 prefix-match hit, remaining 3111 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     364.20 ms\n",
            "llama_perf_context_print: prompt eval time =    4273.32 ms /  3111 tokens (    1.37 ms per token,   728.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1639.77 ms /    52 runs   (   31.53 ms per token,    31.71 tokens per second)\n",
            "llama_perf_context_print:       total time =    7953.21 ms /  3163 tokens\n",
            "llama_perf_context_print:    graphs reused =         50\n",
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [25371]\n",
            "\n",
            "--- End of /tmp/llama_cpp_server.log ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5 — Expose with your RESERVED ngrok domain (always the same URL)\n",
        "from google.colab import userdata\n",
        "from pyngrok import ngrok\n",
        "import time\n",
        "\n",
        "# Your ngrok auth token\n",
        "ngrok.set_auth_token(userdata.get('NGROK_TOKEN'))\n",
        "\n",
        "# Your ngrok reserved domain\n",
        "RESERVED_DOMAIN=(userdata.get('RESERVED_DOMAIN'))\n",
        "\n",
        "# Kill any old tunnels on port 8000 just in case\n",
        "for tunnel in ngrok.get_tunnels():\n",
        "    if tunnel.config.get(\"addr\") == \"localhost:8000\":\n",
        "        ngrok.disconnect(tunnel.public_url)\n",
        "\n",
        "# Open the tunnel with your exact reserved domain\n",
        "tunnel = ngrok.connect(\n",
        "    addr=\"8000\",\n",
        "    proto=\"http\",\n",
        "    bind_tls=True,\n",
        "    domain=RESERVED_DOMAIN          # ← this locks it to your domain\n",
        ")\n",
        "\n",
        "public_url = tunnel.public_url\n",
        "print(\"\\nYOUR PUBLIC ENDPOINT (OpenAI compatible):\")\n",
        "print(public_url)\n",
        "print(\"\\nExample curl:\")\n",
        "print(f'curl {public_url}/v1/chat/completions \\\\\\n'\n",
        "      '  -H \"Content-Type: application/json\" \\\\\\n'\n",
        "      '  -d \\'{{\"model\":\"llama\",\"messages\":[{{\"role\":\"user\",\"content\":\"Hello Stheno!\"}}],\"temperature\":0.8}}\\'')\n",
        "\n",
        "print(\"\\nServer is live at your permanent URL above!\")\n",
        "print(\"You can close the notebook — the tunnel stays alive as long as this cell keeps running.\\n\")\n",
        "\n",
        "# Keep the cell alive forever\n",
        "while True:\n",
        "    time.sleep(60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "collapsed": true,
        "id": "Q_p1cZvUqQ6c",
        "outputId": "10dede51-663d-4150-dc7f-b0e1e0613038"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "YOUR PUBLIC ENDPOINT (OpenAI compatible):\n",
            "https://certain-annually-basilisk.ngrok-free.app\n",
            "\n",
            "Example curl:\n",
            "curl https://certain-annually-basilisk.ngrok-free.app/v1/chat/completions \\\n",
            "  -H \"Content-Type: application/json\" \\\n",
            "  -d '{{\"model\":\"llama\",\"messages\":[{{\"role\":\"user\",\"content\":\"Hello Stheno!\"}}],\"temperature\":0.8}}'\n",
            "\n",
            "Server is live at your permanent URL above!\n",
            "You can close the notebook — the tunnel stays alive as long as this cell keeps running.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3577485463.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Keep the cell alive forever\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}